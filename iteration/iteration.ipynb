{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration and Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENGSCI233: Computational Techniques and Computer Systems** \n",
    "\n",
    "*Department of Engineering Science, University of Auckland*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 What's in this Notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential equations describe how physical systems change over time. If we want to predict those changes into the future, we need to solve the equations. Most can't be solved analytically.\n",
    "\n",
    "The next best option is a numerical method. You have already seen Euler's method - its not very accurate, although the philosophy is spot on. Estimate the derivative, use it to take a jump forward, re-evaluate the derivative. Here, we'll explore some more complex methods, and how to measure their performance.\n",
    "\n",
    "You need to know:\n",
    "- How to use jumps back and forth - predictor and corrector steps - when implementing an improved version of Euler's method. This concept generalises to higher order Runge-Kutta methods.\n",
    "- The accuracy of these methods depends on how large a step we take. We can use a convergence test to decide how small those steps need to be for a decent result.\n",
    "- Sometimes these methods are unstable. That is both a drawback of the method and but also depends on the difficulty of the problem it has been applied to (stiffness)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for user: **equations format poorly** in this notebook. If they are running off the screen \"re-run\" the cell containing the equation (click on the cell, hit Enter, then Ctrl+Enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and environment: this cell must be executed before any other in the notebook\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ODE solution methods\n",
    "\n",
    "<mark>***Different methods to approximately solve Ordinary Differential Equations***</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the initial value problem $\\frac{dy}{dx} = (1 + xy)^{2}, y(0) = 1$.  This equation is **non-separable** and **nonlinear** which makes it difficult to solve.  An approximate solution can still be generated using a numerical method.\n",
    "\n",
    "In this module, we will consider **first-order** ODEs of the form $\\frac{dy}{dx} = f(x,y)$.  Higher-order ODEs can be written as a system of first-order ODEs so this approach can be applied to almost any ODE. Furthermore, we will restrict our interest to **initial value problems**, i.e., cases where information about $y$ (and its derivatives) is known at some starting **reference point** $x^{(0)}$.\n",
    "\n",
    "The numerical methods we will consider are all **step-by-step** methods.  This means that we will start from the **known solution** $y^{(0)}$ at $x^{(0)}$ and proceed **stepwise**.  First, we take a step of length $h$ and **estimate a solution** $y^{(1)}$ at $x^{(1)} = x^{(0)} + h$.  Then, we use the value $y^{(1)}$ as a **pseudo-initial value** to estimate a solution $y^{(2)}$ at $x^{(2)} = x^{(1)} + h = x^{(0)} + 2h$.  Proceeding in this manner we will be able to construct an approximate solution for $y$ over a specified range of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1.1 Euler method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All the methods in this module match the derivatives in a **Taylor's expansion** over a finite region to some order. The omission of **higher-order** Taylor's series terms in the approximation means that each method will have an associated **error**.  The **Euler method** is the simplest in that it only approximates the first derivative of the function. It does this using a first-order **finite difference** approximation to the derivative.\n",
    "\n",
    "The finite difference approximation to the derivative is:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{dy}{dx} \\thickapprox \\dfrac{\\Delta y}{\\Delta x} = \\dfrac{y^{(k+1)} - y^{(k)}}{h} = f\\left(x^{(k)},y^{(k)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "This can be rearranged to give\n",
    "\n",
    "\\begin{equation}\n",
    "  y^{(k+1)} = y^{(k)} + h\\,f\\left(x^{(k)},y^{(k)}\\right) \n",
    "\\end{equation}\n",
    "\n",
    "where $f$ is a known function for the derivative.\n",
    "\n",
    "***Execute the cell below for a demonstration of the Euler method for the ODE $dy/dx = (1+xy)^2$***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from ipywidgets import interact\n",
    "from iteration233 import plot_euler_elements\n",
    "\n",
    "def euler_figure(steps=0.5, h = 0.1):\n",
    "    # create figure\n",
    "    f,(ax1, ax2) = plt.subplots(1,2)\n",
    "    f.set_size_inches(12,5)\n",
    "    \n",
    "    # display plot\n",
    "    plot_euler_elements(ax1, ax2, steps, h)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(euler_figure, steps = (0.5,10,0.5), h = (0.02,0.2,0.02));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***How does error in the approximation change with the step-size, $h$? Describe any trade-offs.***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***What type of error is introduced by truncating the Taylor series to a finite number of terms?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The exercise below attempts to solve the ODE\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dy}{dx} = \\sin\\left(a\\sin(x)\\sqrt{x}+\\frac{\\cos(bx)}{x+1}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "However, we should be mindful of the tradeoff between **efficiency and accuracy** (or effort and error).\n",
    "\n",
    "***Execute the cell below. Experiment with the input boxes.***\n",
    "\n",
    "***In general, how many steps are required to get the error below 5%?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from ipywidgets import fixed\n",
    "from iteration233 import plot_tradeoffs_elements, tradeoffs_setup\n",
    "\n",
    "# function parameters\n",
    "p = [8, 8.5]\n",
    "def tradeoffs_figure(steps, predict_value, p):\n",
    "    # create figure\n",
    "    f,ax = plt.subplots(1,1)\n",
    "    f.set_size_inches([16, 6])\n",
    "    \n",
    "    # display plot\n",
    "    plot_tradeoffs_elements(ax, steps, predict_value, p)\n",
    "\n",
    "# run the interactive figure environment\n",
    "box1,box2 = tradeoffs_setup()\n",
    "interact(tradeoffs_figure, steps = box1, predict_value = box2, p = fixed(p));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1.2 Improved Euler method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Euler method uses the slope (i.e., the value of $y'$) at $y^{(k)}$ to predict $y^{(k+1)}$.  If the slope is **changing significantly** over the step length then the Euler method will give an **inaccurate** solution.  An improved method can be devised by **averaging** the slopes at $y^{(k)}$ and $y^{(k+1)}$.\n",
    "\n",
    "The slope at $y^{(k)}$ is $f\\left(x^{(k)}, y^{(k)}\\right)$. A **prediction** of the solution at $x^{(k+1)}$ is $y^{(k+1)}_p=y^{(k)}+h\\,f\\left(x^{(k)}, y^{(k)}\\right)$. The slope at $y^{(k+1)}$ is $f\\left(x^{(k+1)}, y^{(k+1)}_p\\right)$. \n",
    "\n",
    "Using the average slope in the previous definition of the Euler method yields the **Improved Euler method**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y^{(k+1)} &= y^{(k)} + h \\times \\text{average slope} \\\\\n",
    "&= y^{(k)} + \\dfrac{h}{2} \\left[f\\left(x^{(k)},y^{(k)}\\right) + f\\left(x^{(k)}+h,y^{(k)} + h\\,f\\left(x^{(k)},y^{(k)}\\right)\\right)\\right].    \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "The Improved Euler Method is an example of a **predictor-corrector** method. First, the value $y^{(k+1)}_p$ is predicted using an Euler step. This allows extra slope information to be determined which can be used to make a corrected estimate of $y^{(k+1)}$.\n",
    "\n",
    "The Improved Euler Method is a **second-order** method, i.e., truncation error $\\sim\\mathcal{O}(h^{3})$. The method is computationally **more expensive** than the Euler Method but it provides **more accurate** solutions.\n",
    "\n",
    "***Execute the cell below for a demonstration of the Improved Euler method for the ODE $dy/dx = (1+xy)^2$***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from iteration233 import plot_improved_elements\n",
    "\n",
    "def improved_euler_figure(steps=6.25, compare_euler=False):\n",
    "    # create figure\n",
    "    f,ax = plt.subplots(1,1)\n",
    "    f.set_size_inches([10,4])\n",
    "    \n",
    "    # display plot\n",
    "    plot_improved_elements(ax, steps, compare_euler)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(improved_euler_figure, steps = (0.25,9,0.25), compare_euler = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Runge-Kutta methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you design a better method than improved Euler, that builds complexity in the same manner Euler -> improved Euler? One thing we could do is evaluate a derivative at the mid-point of the step and use that information to compute a **weighted** derivative, e.g.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f_0 & = f\\left(x^{(k)}, y^{(k)}\\right) \\\\\n",
    "f_1 & = f\\left(x^{(k)}+\\frac{h}{2}, y^{(k)}+\\frac{h}{2}f_0\\right) \\\\\n",
    "f_2 & = f\\left(x^{(k)}+h, y^{(k)}-hf_0+2hf_1\\right) \\\\\n",
    "y^{(k+1)} &= y^{(k)} + \\dfrac{h}{6} \\left(f_0 + 4f_1 + f_2\\right).    \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This is a third order scheme with truncation error $\\sim\\mathcal{O}(h^{4})$.\n",
    "\n",
    "The schemes get more complicated, although its diminishing returns beyond 4th order. However, they all fall under a broad category called Runge-Kutta methods. A particularly popular implementation is called **RK45**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Accuracy\n",
    "\n",
    "<mark>***Measuring how close a numerical solution is to the true solution.***</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical computation provides a number of opportunities for **error** to creep in. Improvements to algorithms can **minimize** this error - usually at the **cost** of execution time - but can **never eliminate** it entirely. It is therefore useful to **estimate** the error that accompanies a particular solution method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.1 Euler method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If $y(x^{(k)}+h)$ is the exact value at $x^{(k)}+h$ then, by Taylor's series:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{split}\n",
    "    y\\left(x^{(k)}+h\\right) &= y\\left(x^{(k)}\\right) + h\\,\\frac{dy}{dx}\\left(x^{(k)}\\right) + \\dfrac{h^{2}}{2!} \\frac{d^2y}{dx^2}\\left(x^{(k)}\\right) + \n",
    "    \\dfrac{h^{3}}{3!} \\frac{d^3y}{dx^3}\\left(x^{(k)}\\right)+ \\ldots \\\\\n",
    "    &= \\text{Euler's Method estimate} + \\mathcal{O}(h^{2})\n",
    "  \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "and thus the **truncation error** at each step involves terms that are proportional to $h^{2}$ or higher powers.\n",
    "\n",
    "If we halve the step length then the error per step is proportional to $\\left(\\dfrac{h}{2}\\right)^{2} = \\dfrac{h^{2}}{4}$. However, to get to the same point we must do twice as many steps. Thus the total error is $E \\varpropto 2 \\left(\\dfrac{h^{2}}{4}\\right)= \\dfrac{h^{2}}{2}$. Thus, by **halving the step length we halve the error** (and by doubling the step length we double the error). Because error for Euler's method is proportional to step length, it is termed a **first-order method**. In general, an $n$-th order method will have truncation error of $\\mathcal{O}(h^{n+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.1 Improved Euler method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Improved Euler method is **second order accurate**, i.e., truncation error $\\sim \\mathcal{O}(h^{3})$, but this improvement over the Euler method comes at the cost of **greater computational expense**.\n",
    "\n",
    "From the formulation of the Improved Euler method, the derivative estimated at the **corrector step** can be expanded (applying a first order Taylor expansion, first for $x$, then for $y$)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f\\left(x^{(k)}+h,y^{(k)} + h\\,f\\left(x^{(k)},y^{(k)}\\right)\\right) &=&\\,f\\left(x^{(k)},y^{(k)} + h\\,f\\left(x^{(k)},y^{(k)}\\right)\\right) + h \\frac{\\partial f}{\\partial x}\\left(x^{(k)},y^{(k)} + h\\,f\\left(x^{(k)},y^{(k)}\\right)\\right) \\\\\n",
    "&=&\\,f\\left(x^{(k)},y^{(k)}\\right) +  h\\,f\\left(x^{(k)},y^{(k)}\\right)\\,\\frac{\\partial f}{\\partial y}\\left(x^{(k)},y^{(k)}\\right) + h \\left(\\frac{\\partial f}{\\partial x}\\left(x^{(k)},y^{(k)}\\right) + h\\,f\\left(x^{(k)},y^{(k)}\\right)\\,\\frac{\\partial^2 f}{\\partial x\\partial y}\\left(x^{(k)},y^{(k)}\\right)\\right)  \\\\\n",
    "&=&\\,f\\left(x^{(k)},y^{(k)}\\right) + h \\frac{df}{dx}\\left(x^{(k)}, y^{(k)}\\right) + h^2\\,f\\left(x^{(k)},y^{(k)}\\right)\\,\\frac{\\partial^2 f}{\\partial x\\partial y}\\left(x^{(k)},y^{(k)}\\right),\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where the total derivative is expressed $\\frac{df}{dx}=\\frac{\\partial f}{\\partial x}+\\frac{\\partial y}{\\partial x}\\frac{\\partial f}{\\partial y}$. Substituting this expression into the Improved Euler formula we get\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y^{(k+1)} &=&\\,y^{(k)} + h\\,f\\left(x^{(k)},y^{(k)}\\right) + \\frac{h^2}{2}\\frac{df}{dx}\\left(x^{(k)}, y^{(k)}\\right) + \\mathcal{O}(h^3)\\\\\n",
    "&=&\\,\\text{Improved Euler method estimate} + \\mathcal{O}(h^3).\n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Runge-Kutta methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't bother with a full derivation of truncation error for the higher order methods. It is more important to appreciate the concept: if a method is $n^{th}$-order, then it has truncation error $\\sim\\mathcal{O}(h^{n+1})$. Halving the step size means that error goes from $h^{n+1} \\rightarrow h^{n+1}/2^{n+1}$, but we need to take twice the number of steps, so the total error is $h^{n+1}/2^n$. So the accuracy is improved, but that comes at a cost of having to evaluate more derivatives at ever more midpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Execute the cell below to compare accuracy of the different schemes for different step sizes.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "from iteration233 import plot_accuracy_elements\n",
    "\n",
    "# function parameters\n",
    "def accuracy_figure(more_steps=0):\n",
    "    # create figure\n",
    "    f,(ax,ax2) = plt.subplots(1,2)\n",
    "    f.set_size_inches([12,4])\n",
    "\n",
    "    # display plot\n",
    "    plot_accuracy_elements(ax, ax2, more_steps)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(accuracy_figure, more_steps = (0,15,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What are the slopes of the curves on the RIGHTHAND plot?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Which method is the most accurate?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Stability\n",
    "\n",
    "<mark>***Eliminating the wiggles and wobbles in the numerical solution.***</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Euler method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the simple ODE $y' = ay$, which we know has an exponential solution. For the case that $a<0$, the solution should decay to zero.\n",
    "\n",
    "The Euler update step is \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y^{(k+1)} &=&\\,y^{(k)} + h\\,a\\,y^{(k)} \\\\\n",
    "&=&\\,\\left(1+ha\\right)y^{(k)} \\\\\n",
    "&=&\\,\\left(1+ha\\right)^k y^{(0)}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "If $|1+ha|>1$ then $y^{(k)}$ will keep increasing in value rather than decaying to 0. This defines a **stability criterion** for Euler's method applied to this problem: $-1 < 1+ha < 1 \\Rightarrow 1 + ha > -1 \\Rightarrow ha > -2 \\Rightarrow h < - \\dfrac{2}{a}$.  Therefore the stability criterion for the Euler method to give stable solutions for this problem is $h < -\\dfrac{2}{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Improved Euler method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Improved Euler method is a second-order accurate method and therefore we would expect it to provide **more accurate** solutions than the Euler method for the same choice of step size. However, the stability of the method is **not necessarily improved** as the solution still relies on the Euler estimate of $y_p^{(k+1)}$. \n",
    "\n",
    "The Improved Euler update step is \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y^{(k+1)} &=&\\,y^{(k)} + \\frac{h}{2}\\,\\left(a\\,y^{(k)}+a(1+ha)y^{(k)}\\right) \\\\\n",
    "&=&\\,\\left(1+ha+\\frac{h^2}{2}a^2\\right)y^{(k)} \\\\\n",
    "&=&\\,\\left(1+ha+\\frac{h^2}{2}a^2\\right)^k y^{(0)}.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "For $a<0$ we require $y^{(k)}$ to decrease and not oscillate in sign, which implies $0 < 1 + ha + \\tfrac{h^2}{2} a^2 < 1$. Thus, if \n",
    "\n",
    "\\begin{equation}\n",
    "1 + ha + \\tfrac{h^2}{2} a^2 < 1, \\quad\\Rightarrow\\quad ha ( 1 + \\tfrac{h}{2} a ) < 0 \\quad\\Rightarrow\\quad h < -\\tfrac{2}{a},\n",
    "\\end{equation}\n",
    "\n",
    "which is the **same criterion** for the Euler method. \n",
    "\n",
    "Similarly, if \n",
    "\n",
    "\\begin{equation}\n",
    "1 + ha + \\tfrac{h^2}{2} a^2 > 0 \\quad\\Rightarrow\\quad 2 + 2ha + h^2 a^2 > 0 \\quad\\Rightarrow\\quad ( 1 + ha ) ^2 > -1\n",
    "\\end{equation}\n",
    "\n",
    "which is **always true** since the left-hand-side is positive and the right-hand-side is negative for any choice of $h$. Therefore the criterion for the Improved Euler method to give **stable, non-oscillating** solutions for this problem is $h < -\\tfrac{2}{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Execute the cell below to see how stability depends on method type and step size***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import function\n",
    "from iteration233 import plot_stability_elements\n",
    "\n",
    "# function parameters\n",
    "def stability_figure(steps=28, method='Euler'):\n",
    "    # create figure\n",
    "    f,ax = plt.subplots(1,1)\n",
    "    f.set_size_inches([10,4])\n",
    "\n",
    "    # display plot\n",
    "    plot_stability_elements(ax, steps, method)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(stability_figure, steps = (3,15,1), method = ['Euler','Improved Euler','RK45']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set STEPS to 15. Which method is the most accurate?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***For the equation above, $a$=-10. For what step size does the Euler method exhibit oscillation?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Set STEPS to 6. Which method is the most accurate?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***For what step size does the Euler method exhibit instability?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stiffness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above illustrates a quality of some differential equations called **stiffness**. While the definition of stiffness is a little nebulous, the following characteristics can be used to diagnose stiff solutions:\n",
    "- Method stability is **constrained** by step size.\n",
    "- Derivatives depend strongly on, or are large compared to, the solution.\n",
    "- Unnecessarily **expensive** with explicit methods like Euler, improved Euler, RK45. Performs better with **implicit** methods (next year).\n",
    "\n",
    "Stiffness is a **property of the solution** whereas stability is a property of the **solution method**. Consider the example below\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dy}{dx}=a(x)y,\\quad\\quad\\begin{Bmatrix} & 0.1 \\quad & x < 20 \\\\ a(x) = & 1\\quad & 20\\leq x < 40 \\\\ & 0.1 \\quad & x \\geq 40 \\end{Bmatrix},\\quad\\quad y(0)=1\n",
    "\\end{equation}\n",
    "\n",
    "***Execute the cell below for a demonstration of solution stiffness.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "from iteration233 import plot_stiffness_elements\n",
    "\n",
    "# function parameters\n",
    "def stiffness_figure(step_size=0.5, log_yscale=False):\n",
    "    # create figure\n",
    "    f,(ax,ax2) = plt.subplots(1,2)\n",
    "    f.set_size_inches([12,5])\n",
    "\n",
    "    # display plot\n",
    "    plot_stiffness_elements(ax, ax2, step_size)\n",
    "    if log_yscale: ax.set_yscale('symlog',linthreshy=1.e-5)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(stiffness_figure, step_size = (0.75,2.5,0.25), log_yscale=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set the STEP_SIZE to 0.75. Does the Euler method to a reasonable job solving this problem?*** \n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Set the STEP_SIZE to 1.75. Where have solution oscillations started to appear? Use the righthand plot to explain the appearance of solution oscillations in terms of the step size and value of $a$.*** \n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Set the STEP_SIZE to 2.75. Where is the solution exhibiting stiffness? Use the righthand plot to explain how ONLY PART of the solution can be stiff. Explain the stiffness in terms of the step size and value of $a$.*** \n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Convergence\n",
    "\n",
    "<mark>***Determining when a numerical solution is close enough to the true solution.***</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most ODEs do not have an **analytical solution**. How should we determine that a solution is **sufficiently accurate** when the true solution can never be known? One way is to solve the ODE for a range of step sizes, $h$, and look for **convergent behaviour**. \n",
    "\n",
    "For example, take the ODE in Section 3 with $a=10$, $y(0) = 1$ and say we wish to estimate $y(0.1)$\n",
    "\n",
    "***Execute the cell below for a demonstration of ODE convergence.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "from iteration233 import plot_convergence_elements\n",
    "\n",
    "# function parameters\n",
    "def convergence_figure(more_steps=0, zoom=False):\n",
    "    # create figure\n",
    "    f,(ax,ax2) = plt.subplots(1,2)\n",
    "    f.set_size_inches([12,4])\n",
    "\n",
    "    # display plot\n",
    "    plot_convergence_elements(ax, ax2, more_steps, zoom)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(convergence_figure, more_steps = (0,15,1), zoom = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set MORE_STEPS to 3. How has the ODE solution at $x=0.2$ changed by taking smaller steps?*** \n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***What does the horizontal axis on the righthand plot represent?*** \n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Set MORE_STEPS to 15. What value is the solution converging toward? (Use the ZOOM if you need to.)*** \n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***How should we quantify and monitor for convergent behaviour?*** \n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Calculate the true solutions for $y(0.2)$ - how do the estimates above compare?*** \n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***The [Bulirsch-Stoer](https://en.wikipedia.org/wiki/Bulirsch%E2%80%93Stoer_algorithm) algorithm fits a function to the points on the RIGHTHAND plot and extrapolates the result at $h=0$. What value does this correspond to on the horizontal axis?*** \n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
