{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENGSCI233: Computational Techniques and Computer Systems** \n",
    "\n",
    "*Department of Engineering Science, University of Auckland*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 What's in this Notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having mostly bug-free code is a **necessary but not sufficient** condition for a good computer program. This notebook is all about understanding code performance, how algorithms scale with larger and larger problems, how to identify bottlenecks, and strategies to speed up execution.\n",
    "\n",
    "Things you need to know:\n",
    "- That Big O notation measures how algorithm runtime grows with problem size. You can read this from a graph by running the algorithm several times at different problem sizes.\n",
    "- Profiling tells you how long your code spends running different functions, so you can figure out which parts are too slow.\n",
    "- When calculations are independent of each other, they can be run in parallel. This gives you speed-up and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and environment: this cell must be executed before any other in the notebook\n",
    "%matplotlib inline\n",
    "from performance233 import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1 Measuring algorithms\n",
    "\n",
    "<mark>***How should we decide if one algorithm is better than another?***</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One way to assess an algorithm is to count how many operations it takes to solve a problem of size $N$, and to compare this against another implementation. For instance, suppose we wish to sort an array of $N$ random numbers in ascending order; we could use:\n",
    "\n",
    "1. Heapsort, which takes $k_1N\\log_2 N+k_2$ operations to complete,\n",
    "2. or insertion sort, which takes $k_3 N^2 + k_4$ operations to complete.\n",
    "\n",
    "Depending on the values of $[k_1, k_2, k_3, k_4]$, we may assess one algorithm as superior to the other for a given problem size. \n",
    "\n",
    "A second approach is to compare the asymptotic scaling of each algorithm: how do they perform as $N$ gets **really really large**. The latter we denote using $\\mathcal{O}()$, called [\"Big O notation\"](https://www.interviewcake.com/article/java/big-o-notation-time-and-space-complexity) or order-of-complexity. This notation ignores **constant multiplicative factors** and focuses on the functional form. \n",
    "\n",
    "From a scaling perspective, heapsort with order-of-complexity $\\mathcal{O}(N\\log_2N)$ is superior to insertion sort with $\\mathcal{O}(N^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.1 Graphing $\\mathcal{O}()$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How do we decide if an algorithm is $\\mathcal{O}(N^2)$ or $\\mathcal{O}(N\\log_2N)$? Both have graphs that are concave up...\n",
    "\n",
    "***Execute the cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "compare_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A useful diagnostic is to plot execution time on log-log axes, for a few doublings of the problem size, i.e., $N$, $2N$, $4N$, $8N$, etc. If the problem has scaling $\\mathcal{O}(N^\\alpha)$, then the plot will be linear on log-log axes, and $\\alpha$ can be read off as the slope.\n",
    "\n",
    "***Execute the cell below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scaling_loglog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***Read the slope off the middle plot and verify that it is 2, i.e., $\\mathcal{O}(N^2)$. ***\n",
    "\n",
    "***There are two-ways to construct a log-log plot: (middle) by explicitly log converting the $x$ and $y$ quantities, and (right) by calling the Python commands `ax.set_xscale('log')` and `ax.set_yscale('log')`.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.2 Other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Scaling of the execution time** is just one consideration when deciding on an algorithm implementation. Depending on the application or hardware available, consideration may also have to be given to memory use, stability, or preconditions of the algorithm input (some search algorithms are *very* fast if the input is already sorted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2 Profiling for optimisation\n",
    "\n",
    "<mark>***How should we choose which parts of a computer program should be improved?***</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Large computer programs might comprise hundreds of different functions or methods calling each other in a complex sequence. Just one poorly written implementation can **slow the entire program**. How can we locate these bottlenecks?\n",
    "\n",
    "Profilers are used to automatically analyse the performance of our code. This could include how efficiently is uses memory or how quickly it performs tasks. Here, we will focus on the latter, introducing the idea of code execution time and looking at how a profiler can be used to generate **execution time** statistics for a computer program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.1 Measuring time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ultimately, profiling relies on measuring **how long it takes** to run different parts of the code. So we need a measure of [`time`](https://www.programiz.com/python-programming/time#time).\n",
    "\n",
    "***Run the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import the time module\n",
    "from time import time\n",
    "\n",
    "# get the current time IN SECONDS from the system clock\n",
    "t0 = time()              \n",
    "print(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***Run the cell above over and over (Ctrl+Enter ***\\*wait\\* *** Ctrl+Enter ***\\*wait\\* *** Ctrl+Enter). How does the output change?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Divide `t0` by 3600, to print the number of hours. Convert this to a number of days and a number of years.***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Subtract the current time IN YEARS from today's date. When did the clock start?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Once we can measure time, we can measure **time differences**.\n",
    "\n",
    "***Run the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start the clock\n",
    "tstart = time()\n",
    "\n",
    "# do something you want to time\n",
    "    # e.g., find the location of the largest element in an array of random numbers\n",
    "i = np.argmax(np.random.rand(1000))\n",
    "    \n",
    "# stop the clock\n",
    "tend = time()\n",
    "print(tend-tstart, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***How long does it take to find the max value? Do you believe the answer?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Run the cell above over and over (Ctrl+Enter ***\\*wait\\* *** Ctrl+Enter ***\\*wait\\* *** Ctrl+Enter). How does the output change?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sometimes an operation executes **too quickly** to be timed accurately using `time()`. In which case, we can repeat the operation `N` times, and divide the total execution time by `N`.\n",
    "\n",
    "***Run the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start the clock\n",
    "tstart = time()\n",
    "\n",
    "# do something you want to time\n",
    "    # e.g., find the location of the largest element in an array of random numbers\n",
    "N = 100\n",
    "for j in range(N):\n",
    "    i = np.argmax(np.random.rand(1000))\n",
    "    \n",
    "# stop the clock\n",
    "tend = time()\n",
    "print((tend-tstart)/N, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***How long does it take to find the max value? Does this agree with the previous estimate?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Increase the value of `N`. Does the estimate of the execution time change (or at least stabilise?)***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.2 Profilers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So, one way to get a sense of where our code is slow, is by writing in a whole bunch of `time()` and `print()` calls. This takes forever when you have a complicated code... and then you just have to go back in and pull them out when you're finished optimising.\n",
    "\n",
    "It's much better to use a ***PROFILER***. In the lab, you will use the [`cProfile`](https://docs.python.org/3.2/library/profile.html) module to study the efficiency of an LU factorisation implementation. There is not much more to say here except to study the typical output of such a profiler.\n",
    "\n",
    "<img src=\"img/profiler.png\" alt=\"Drawing\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There's a lot of useful information to unpack here. After some general header information (e.g., total runtime) the profiler goes on to rank different function and method calls by the total time the code has spend in them. The columns give:\n",
    "\n",
    "1. The **total number** of times the function or method was called.\n",
    "2. The **total time** spent in that function or method (**excludes other function or method calls**).\n",
    "3. Total time **per function or method call** (excludes time spent calling other functions or methods).\n",
    "4. Cumulative time spent in that function or method - `tend-tstart` in the example above (**includes other functions of methods**).\n",
    "5. Cumulative time per function or method call - `(tend-tstart)/N` in the example above (includes time spent calling other functions of methods).\n",
    "6. The **name** of the function or method.\n",
    "\n",
    "From the print out above, we can identify that the large majority of time is spent in the `row_reduction` function, and this is a consequence of ***both*** the large number of function calls (199) ***and*** the relatively slow function execution (0.043 seconds, compared to the next slowest `lu_factor` at 0.005 seconds per call). \n",
    "\n",
    "Perhaps we should take a look at improving `row_reduction`? You'll do this in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Concurrency and Parallelisation\n",
    "\n",
    "<mark>***How can we make optimised code even faster?***</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the 80's and 90's, clever engineers devised new methods to squeeze more and more transistors onto microchips. The result was that computing speeds roughly **doubled every 2 years**: the so-called [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law). While research into transistor miniaturisation [continues to this day](https://www.technologyreview.com/s/602592/worlds-smallest-transistor-is-cool-but-wont-save-moores-law/), in practice, gains in computing power are achieved through **multi-core processing**. Many desktops now come with 8-core chips (8 independent processors) as standard, although if you're rereading these notes in 5 years time that will probably [sound primitive](https://i.pinimg.com/originals/93/44/66/9344663cd0094039d4bacd47f67d48fe.jpg). \n",
    "\n",
    "**Concurrency** is the idea that you can do two or more things at the same time<sup>2</sup>. It is ubiquitous in computing: multiple apps ***concurrently*** running on your phone, 30 or so ENGSCI students ***concurrently*** working through some contrived lab problem on a networked desktop machine each Wednesday morning. In each case, we can think of a **shared resource** (your phone's memory, a pool of Desktops) being accessed by **independent processes** (phone apps, ENGSCI nerds). \n",
    "\n",
    "The same concepts can be applied to **parallelise** your code.\n",
    "\n",
    "<sup>2</sup><sub>[Scary concept if you're a male.](https://vignette.wikia.nocookie.net/tehmeme/images/5/5d/Y0UJC.png/revision/latest?cb=20120505151500)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 An example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***- - - - CLASS CODING EXERCISE - - - -***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take Python to factorise a 3000$\\times$3000 matrix? How about 10 of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some pacakges\n",
    "from scipy.linalg import lu\n",
    "import numpy as np\n",
    "from time import time\n",
    "    \n",
    "N = 3000\n",
    "n = 10\n",
    "\n",
    "# create some matrices\n",
    "As = []\n",
    "for i in range(n):\n",
    "    As.append(np.random.rand(N,N))\n",
    "\n",
    "# factorise one matrix using lu()\n",
    "t0 = time()\n",
    "P,L,U = lu(As[0])\n",
    "t1 = time()\n",
    "print('factorising 1 matrix: ',t1-t0, 'seconds')\n",
    "\n",
    "# factorise ten matrices using lu()\n",
    "# *** your code here ***\n",
    "\n",
    "print('factorising {:d} matrices: '.format(n),t1-t0, 'seconds')\n",
    "\n",
    "# free up the memory \n",
    "del(As)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Does the time for factorising 1 matrix vs. 10 matrices scale as you expect?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Explain whether we need to have FINISHED factorising the FIRST matrix before starting on the SECOND.***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Thinking about parallelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-core microprocessor contains several independent processing units. We can think of these as forming a **pool** of workers. At any given time, some of the workers may be **idle** while others will be busy completing an **assigned task**. When a new request comes along, it will be either assigned to an available worker or, in the event everyone is busy, stored in a **queue**. When a worker finishes a task, they are **returned to the pool** ready to receive the next queued assignment.\n",
    "\n",
    "***If the input to one assignment does not depend on the outcome of another, then two workers can complete their tasks simultaneously***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case is relatively easy to treat in Python using the [`multiprocessing`](https://docs.python.org/3.4/library/multiprocessing.html?highlight=process) library, the [`Pool`](https://docs.python.org/3.4/library/multiprocessing.html?highlight=process#multiprocessing.pool.Pool) class, and the [`map()`](https://docs.python.org/3.4/library/multiprocessing.html?highlight=process#multiprocessing.pool.Pool.map) method. Although, we cannot do multiprocessing inside this Jupyter Notebook, I have included a supplementary script `parallel_example.py` that demonstrates how to parallelise a loop and what kind of speed-ups can be achieved.\n",
    "\n",
    "<img src=\"img/parallel_example.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "***Visual Studio Code printout for `parallel_example.py`, a parallelised version of Example 3.1.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if the two assignments are **related to one another**, two workers may still be able to complete them simultaneously, although they may need to **communicate with each other** from time to time. To treat this case, we require a [message passing protocol](https://en.wikipedia.org/wiki/Message_passing). You won't need to do that in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Speed-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of parallelisation is to reduce the execution time of a program. By measuring execution time for different sized pools, we can develop a sense of relative gains and diminishing returns. We define the parallel speedup, $S$, and parallel efficiency, $E$,\n",
    "\n",
    "\\begin{equation}\n",
    "S=\\frac{T_s}{T_p},\\quad\\quad E = \\frac{S}{n_p}\n",
    "\\end{equation}\n",
    "\n",
    "where $T_s$ is the **serial execution time** and $T_p$ is the parallel execution time for a pool of size $n_p$.\n",
    "\n",
    "***Run the cell below to plot parallel speed-up and efficiency for `parallel_example.py`.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What does the phrase \"linear speedup\" imply?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Which sections of the lefthand plot would you consider \"sublinear\" and \"supralinear\"?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Explain how the parallel speedup plot shows diminishing returns.***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Why might the speedup get WORSE for a very large pool?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution code for 3.1\n",
    "#t0 = time()\n",
    "#for A in As:\n",
    "#    P,L,U = lu(A)\n",
    "#t1 = time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
