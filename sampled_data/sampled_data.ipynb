{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENGSCI233: Computational Techniques and Computer Systems** \n",
    "\n",
    "*Department of Engineering Science, University of Auckland*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This topic addresses algorithmic implementations of **interpolation** and **integration**.\n",
    "\n",
    "Interpolation is when we try to estimate the value of some process using observations around it. There are many ways to do this: drawing lines between data points, or fitting smooth functions like polynomials or cubic splines.\n",
    "\n",
    "Integration is when we try to estimate the area under a curve or, sometimes, under discrete data. Methods include the trapezium rule and Gaussian quadrature.\n",
    "\n",
    "You need to know:\n",
    "- The pros and cons of linear, polynomial and spline interpolation, and how these extend to extrapolation.\n",
    "- The math of trapezium rule and Gaussian quadrature. In all cases, a weighted sum of function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and environment: this cell must be executed before any other in the notebook\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampled data is defined as ***a set of measurements of a continuous process at discrete points (locations or times)***. \n",
    "\n",
    "We shall use the following terminology. \n",
    " - The **continuous process** is $y(x)$, with $x$ the **independent variable** (e.g., position, time) and $y$ the **dependent variable** (e.g., velocity, temperature, force).\n",
    " - The set of $N$ **measurement points** are $[x_0,x_1,\\cdots x_{n-1}]$ or, more compactly, $x_i$, where the index, $i$, indicates \"*could be any of the values 0 through to $n-1$*\".\n",
    " - The corresponding set of $N$ **measurements values** are $[y(x_0), y(x_1),\\cdots y(x_{n-1})]$, or $[y_0, y_1,\\cdots y_{n-1}]$, or $y_i$.\n",
    "\n",
    "As an example, consider the set of eight temperature measurements (points and values) in the table below:\n",
    "\n",
    "|  time  |  2.5 | 3.5  | 4.5 |  5.6 |  8.6 |  9.9 | 13.0  | 13.5|\n",
    "|-|-|-|-|-|-|-|-|-|-|\n",
    "| temperature |24.7 | 21.5 | 21.6 | 22.2 | 28.2 | 26.3 | 41.7 | 54.8|\n",
    "\n",
    "In this case: $N=8$; the dependent variable is temperature, $T$; and the independent variable is time, $t$, i.e., $y(x)\\equiv T(t)$. Note, there is no requirement that the measurement points are evenly spaced.\n",
    "\n",
    "***Run the code in the cell below to view the data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sampled_data233 import plot_data\n",
    "\n",
    "# specify the data as numpy arrays\n",
    "xi = np.array([2.5, 3.5, 4.5, 5.6, 8.6, 9.9, 13.0, 13.5])\n",
    "yi = np.array([24.7, 21.5, 21.6, 22.2, 28.2, 26.3, 41.7, 54.8])\n",
    "\n",
    "# plot the data\n",
    "f,ax = plt.subplots(1,1)\n",
    "plot_data(xi,yi,ax,label='data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampled data $(x_i,y_i)$ provide an **incomplete picture** of the continuous process $y(x)$. We shall consider two uses of such data: **interpolation** and **integration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## [1 Interpolation](https://en.wikipedia.org/wiki/Interpolation)\n",
    "\n",
    "<mark>***Filling in the gaps between data.***</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Often we may need a measurement at a particular point or points, $x_j$, for which there are no data (note, I am using the index $i$ for data we **do have**, and $j$ for data we **don't**). However, if there is data **either side** of the unknown point, then we can use this information to approximate $y_j=y(x_j)$: this is known as **interpolation**. \n",
    "\n",
    "***Run the code in the cell below to see proposed interpolation points, $x_j$.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sampled_data233 import plot_interpolation_lines\n",
    "\n",
    "# specify the interpolation locations\n",
    "xj = np.linspace(3., 13., 11)\n",
    "\n",
    "# plot the data\n",
    "f,ax = plt.subplots(1,1)\n",
    "plot_data(xi,yi,ax,label='data')\n",
    "\n",
    "# plot interpolation locations as vertical lines\n",
    "plot_interpolation_lines(xj, ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***Modify the code above so that interpolation is twice as frequent.***\n",
    "\n",
    "We shall consider three methods of interpolation. They are all variations on the same strategy:\n",
    "1. Find an **interpolating function**, $f(x)$, that exactly or approximately matches the data, i.e., $y_i= \\text{or}\\approx f(x_i)$.\n",
    "2. Find $y_j$ by **evaluating** $f(x)$ at the points of interest, i.e., $y_j=f(x_j)$.\n",
    "\n",
    "The three methods - (i) polynomial fitting, (ii) piecewise linear interpolation, and (iii) cubic splines - differ only in how they **determine** $f(x)$.\n",
    "\n",
    "Note, we are assuming the $x_j$ lies within the range of our data, i.e., $x_0\\leq x_j\\leq x_{n-1}$. If $x_j$ lies outside this range, the process is called **extrapolation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### [1.1 Polynomial fitting](https://en.wikipedia.org/wiki/Polynomial_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You have probably already encountered this type of interpolation. For example, finding the **line of best fit** is an example of fitting a polynomial of order 1 to a set of points. \n",
    "\n",
    "Suppose the interpolating function is a polynomial, e.g., $f(x)=3x^4-2x^2+2$ or $T=f(t)=-8.35t +1.06\\times 10^4$. We need to determine:\n",
    "- What **order** should the polynomial be? (What is the highest power of the independent variable?)\n",
    "- How should I find its **coefficients**?\n",
    "\n",
    "The first point is a judgment call on your part. As we shall see, you are trading off polynomial complexity against the ability to fit the data. As a general rule, you should ***seek the simplest polynomial that does a \"good job fitting the data\"***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.1.1 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A polynomial of order $m$ has the form\n",
    "\n",
    "$$ f(x) = \\sum\\limits_{k=0}^{m} a_k x^k $$\n",
    "\n",
    "where we use the index $k$ to distinguish from the data index, $i$, and the interpolation index, $j$. The coefficients are denoted by $a_k$, e.g., for the polynomial $y=2x^2-3x+1$, $a_k=[2,-3,1]$. \n",
    "\n",
    "We need to find the values of $a_k$ that provide the \"**best fit to the data**\" - let's define that phrase.\n",
    "\n",
    "We **know** that the data has values $y_i$ (we measured those). The interpolating function **\"predicts\"** that the data should have values $f(x_i)$. Generally, these will **not** be equal, i.e., $y_i \\neq f(x_i)$.\n",
    "\n",
    "***\"Doesn't that make it a terrible interpolating function?\"***\n",
    "\n",
    "Not necessarily, providing that the $f(x_i)$ is **reasonably close** to $y_i$. Formally, we define how close they are by (i) taking the difference between $f(x_i)$ and $y_i$, (ii) squaring it (so there is no difference between under and overpredicting), and (iii) adding the squared differences for all data points (to obtain a single number). Thus, the **sum of squared differences** is given\n",
    "\n",
    "$$ R^2 = (y_0-f(x_0))^2+(y_1-f(x_1))^2+\\cdots (y_{n-1}-f(x_{n-1}))^2 = \\sum\\limits_{i=0}^{n-1}\\left(y_i-(a_0+a_1x_i+a_2x_i^2+\\cdots a_mx_i^m)\\right)^2 $$\n",
    "\n",
    "This single number, sometimes called the **residual**, is a quantitative measure of \"*how good does the interpolating function match the data*\".\n",
    "\n",
    "The next goal is to find values of the coefficients, $a_k$, that minimize $R^2$. Generally speaking, how do we find $x$ that minimises $y(x)$? Why, by finding the value $x$ where $dy/dx=0$ of course! (And maybe checking the second derivative is >0 to prove this is a minimum.)\n",
    "\n",
    "Given there are $m$ coefficients for us to find, this means we can form equations for $m$ derivatives (one each with respect to the $m$ coefficients $a_k$) and solve these simultaneously. For example, for the case $m=2$, we have $f(x)=a_0+a_1x+a_2x^2$, and we can formulate three equations:\n",
    "\n",
    "$$ \\frac{\\partial(R^2)}{\\partial a_0}=-2 \\sum\\limits_{i=0}^{n-1}\\left(y_i - (a_0+a_1x_i+a_2x_i^2)\\right)=0,$$\n",
    "\n",
    "$$ \\frac{\\partial(R^2)}{\\partial a_1}=-2 \\sum\\limits_{i=0}^{n-1} x_i \\left(y_i - (a_0+a_1x_i+a_2x_i^2)\\right)=0,$$\n",
    "\n",
    "$$ \\frac{\\partial(R^2)}{\\partial a_2}=-2 \\sum\\limits_{i=0}^{n-1} x_i^2\\left(y_i - (a_0+a_1x_i+a_2x_i^2)\\right)=0.$$\n",
    "\n",
    "***Confirm that you can obtain these equations: differentiate the expression for $R^2$ with respect to the coefficients, $a_k$, and set equal to zero (you will need to use the chain rule and hold the $x_i$ constant).***\n",
    "\n",
    "Each of these equations can be rearranged in terms of (1) a LHS that depends on the unknown coefficients, $a_k$, and (2) a RHS that depends only on the known data, $x_i$ and $y_i$. For example, the first equation can be rewritten as:\n",
    "\n",
    "$$ (n)a_0 + \\left(\\sum x_i\\right) a_1 + \\left(\\sum x_i^2\\right)a_2 = \\sum y_i  $$\n",
    "\n",
    "We can write the three equations using the **Vandermonde matrix** for the LHS:\n",
    "\n",
    "$$ \\begin{bmatrix} n \\quad \\sum x_i \\quad \\sum x_i^2 \\\\ \\sum x_i \\quad \\sum x_i^2 \\quad \\sum x_i^3 \\\\ \\sum x_i^2 \\quad \\sum x_i^3 \\quad \\sum x_i^4\\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} \\sum y_i \\\\ \\sum x_iy_i \\\\ \\sum x_i^2y_i \\end{bmatrix}.  $$\n",
    "\n",
    "This system of equations is now in the form $A\\mathbf{x}=\\mathbf{b}$ and can be solved (numerically) using the LU factorization algorithm developed last week.\n",
    "\n",
    "For an $m$ order polynomial, the expression above generalizes:\n",
    "\n",
    "$$ \\begin{bmatrix} \n",
    "&n& \\quad &\\sum x_i& \\quad &\\sum x_i^2& \\quad &\\cdots& \\quad &\\sum x_i^m& \\\\ \n",
    "&\\sum x_i& \\quad &\\sum x_i^2& \\quad &\\sum x_i^3&  \\quad &\\cdots& \\quad &\\sum x_i^{m+1}& \\\\\n",
    "&\\sum x_i^2& \\quad &\\sum x_i^3& \\quad &\\sum x_i^4& \\quad &\\cdots& \\quad &\\sum x_i^{m+2}& \\\\\n",
    "&\\vdots& \\quad &\\vdots& \\quad &\\vdots& \\quad &\\ddots& \\quad &\\vdots& \\\\\n",
    "&\\sum x_i^m& \\quad &\\sum x_i^{m+1}& \\quad &\\sum x_i^{m+2}& \\quad &\\cdots& \\quad &\\sum x_i^{2m}& \\end{bmatrix} \n",
    "\\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m\\end{bmatrix} = \\begin{bmatrix} \\sum y_i \\\\ \\sum x_iy_i \\\\ \\sum x_i^2y_i \\\\ \\vdots \\\\ \\sum x_i^m y_i \\end{bmatrix}.  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.1.2 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The key algorithm steps for polynomial fitting are:\n",
    "```\n",
    "1. Initialise: polynomial order M, data XI, YI.\n",
    "2. Construct size M+1 square Vandermonde matrix using data XI.\n",
    "3. Construct length M+1 RHS vector using data XI, YI.\n",
    "4. Solve Ax=b for coefficients of f(x).\n",
    " - compute LU decomposition of A using LU_factor\n",
    " - find x using LU_solve\n",
    "```\n",
    "Once the coefficients of $f(x)$ are determined, interpolation proceeds straightforwardly:\n",
    "```\n",
    "5. Initialise: interpolation locations, XJ.\n",
    "6. Evaluate f(x) at XJ.\n",
    "```\n",
    "A completed Python implementation of polynomial fitting is given in `sampling233.py` and demonstrated in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.1.3 Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from ipywidgets import interact, fixed\n",
    "from sampled_data233 import plot_polynomial_elements\n",
    "\n",
    "def polynomial_fitting_figure(order=1, interpolate = False, extrapolate = False, xi = None, yi = None, xj = None):\n",
    "    # create figure\n",
    "    f,ax = plt.subplots(1,1)\n",
    "    f.set_size_inches(5,5)\n",
    "    \n",
    "    # see sampling233.py for details on the Python implementation of polynomial fitting\n",
    "    plot_polynomial_elements(ax, xi, yi, xj, order, interpolate, extrapolate)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(polynomial_fitting_figure, order = (1,8,1), interpolate = False, extrapolate = False, xi = fixed(xi), yi = fixed(yi), xj = fixed(xj));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# EXPERIMENT with the order of the fitted function above to obtain\n",
    "# the BEST polynomial.\n",
    "\n",
    "# To determine BEST, weigh the CRITERIA below:\n",
    "# - MISFIT with the data\n",
    "# - PLAUSIBILITY of the equation\n",
    "# - ability to EXTRAPOLATE\n",
    "\n",
    "# ENTER your answer in POLYNOMIAL FITTING ORDER poll on the MODULE PAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.1.4 Concept questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***Compare the interpolating function for $m=2$ and $m=6$:***\n",
    "- *Which fits the data better?*\n",
    "- *Which looks more like a physical process?*\n",
    "- *Which is the better choice for $m$? If you chose $2$, how do you justify not matching the data?*\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***What value of $m$ ensures that the $R^2$ is (almost) zero? How does this relate to the number of data points?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***When $R^2=0$, $f(x)$ passes through all the data points exactly. Why might this be a bad thing?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Why do we encounter problems when setting $m=8$.***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Can $f(x)$ be used for extrapolation? (i.e., is $f(x)$ defined for $x_j<x_0$ or $x_j>x_{n-1}$?)***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### [1.2 Piecewise linear interpolation](https://en.wikipedia.org/wiki/Linear_interpolation#Linear_interpolation_between_two_known_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This method **fits a straight line segment between neighbouring data points**. For $N$ data points, there are $N-1$ adjacent data pairs, and the set of $N-1$ straight line segments defines the interpolating function, $f(x)$, in a **piecewise** manner.\n",
    "\n",
    "This interpolation method is commonly used and is easy to implement. It is one of a subset of methods called **Lagrange interpolation**.\n",
    "\n",
    "Piecewise linear interpolation adopts the same perspective of sampled data as the **finite difference** formula and the **Euler method**, i.e., \"*we don't know what happens between these two points, let's assume its a straight line.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.2.1 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define the $i^{\\text{th}}$ **subinterval** between neighbouring data points, $I_i=[x_i, x_{i+1}]$. The straight line linking these points is given by:\n",
    "\n",
    "$$y(x) = y_i + \\frac{y_{i+1}-y_i}{x_{i+1}-x_i}(x-x_i) = m_ix+c_i$$\n",
    "\n",
    "where $m_i=(y_{i+1}-y_i)/(x_{i+1}-x_i)$ and $c_i=y_i-m_ix_i$ i.e., the gradient and intercept of the straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.2.2 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The key algorithm steps for piecewise linear interpolation are:\n",
    "```\n",
    "1. Initialise data XI, YI.\n",
    "2. For the ith subinterval, compute the straight line gradient, mi, and intercept, ci.\n",
    "3. Find which interpolation points, xj, fall within the subinterval, i.e., xi<xj<xi+1.\n",
    "4. Evaluate the piecewise interpolating function at xj.\n",
    "```\n",
    "A Python implementation of piecewise linear interpolation is given in `sampling233.py` and demonstrated in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.2.3 Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sampled_data233 import plot_piecewise_elements\n",
    "\n",
    "def piecewise_linear_figure(interpolate = False, xi = None, yi = None, xj = None):\n",
    "    # create figure\n",
    "    f,ax = plt.subplots(1,1)\n",
    "    f.set_size_inches(5,5)\n",
    "    \n",
    "    # see sampling233.py for details on the Python implementation of piecewise linear interpolation\n",
    "    plot_piecewise_elements(ax, interpolate, xi, yi, xj)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(piecewise_linear_figure, interpolate = False, xi = fixed(xi), yi = fixed(yi), xj = fixed(xj));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.2.4 Concept questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***The interpolating function passes through each point exactly. Explain when this could be a disadvantage.***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***$f(x)$ is continuous at the data points. Are its derivatives?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Can $f(x)$ be used for extrapolation?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***- - - - CLASS CODING EXERCISE - - - -***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# PART ONE\n",
    "# --------\n",
    "# SUPPOSE we have the interpolating function\n",
    "def interp(x0,y0,x1,y1,x): \n",
    "    return y0 + (y1 - y0)/(x1 - x0)*(x - x0)\n",
    "# and the data XI and YI\n",
    "xi = np.array([2.5, 3.5, 4.5, 5.6, 8.6, 9.9, 13.0, 13.5])\n",
    "yi = np.array([24.7, 21.5, 21.6, 22.2, 28.2, 26.3, 41.7, 54.8])\n",
    "\n",
    "# for example\n",
    "x = 3\n",
    "y = interp(xi[0], yi[0], xi[1], yi[1], x)\n",
    "print(x,y)\n",
    "\n",
    "# For the interpolation value X below, loop over PAIRS of data points in XI\n",
    "# to FIND the interval that X lies inside of, then perform the interpolation\n",
    "x = 6.8\n",
    "# **your code here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL CHALLENGE\n",
    "# -----------------\n",
    "# Do the same thing, but for \n",
    "xj = [3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### [1.3 Cubic splines](https://en.wikipedia.org/wiki/Spline_interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to overcome **derivative discontinuity** inherent in piecewise linear interpolation (and higher order Lagrange schemes), cubic splines are used. Like the previous method, for $N$ data points, spline based interpolation breaks the range of the interpolating function into $N-1$ subintervals of the form $I_i = [x_i , x_{i+1}]$. A **different** cubic polynomial, **passing through the data exactly**, is then used to interpolate within each subinterval. \n",
    "\n",
    "The polynomials are chosen so that $f(x)$, and its **first and second derivatives are continuous** at the subinterval boundaries, $x_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.3.1 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the $i^{\\text{th}}$ subinterval, define the cubic polynomial, $p_i(x)$, with coefficients\n",
    "\n",
    "$$p_i(x) = a_0^{(i)} + a_1^{(i)}(x − x_i) + a_2^{(i)}(x − x_i)^2 + a_3^{(i)}(x − x_i)^3.$$\n",
    "\n",
    "Note, the superscript $(i)$ means \"*this coefficient belongs to the $i^{\\text{th}}$ subinterval*\", not \"*to the power $i$*\".\n",
    "\n",
    "Similarly for the neighbouring subintervals ($i-1$ and $i+1$), we can write\n",
    "\n",
    "$$p_{i-1}(x) = a_0^{(i-1)} + a_1^{(i-1)}(x − x_{i-1}) + a_2^{(i-1)}(x − x_{i-1})^2 + a_3^{(i-1)}(x − x_{i-1})^3.$$\n",
    "\n",
    "$$p_{i+1}(x) = a_0^{(i+1)} + a_1^{(i+1)}(x − x_{i+1}) + a_2^{(i+1)}(x − x_{i+1})^2 + a_3^{(i+1)}(x − x_{i+1})^3.$$\n",
    "\n",
    "**How many unknowns?**\n",
    "\n",
    "As with polynomial fitting, there are a certain number of unknown polynomial coefficients. We shall set up an equal number of equations and solve these simultaneously. **How many unknowns?** Well, there are $N-1$ subintervals (for $N$ data points there are $N-1$ neighbouring pairs), and *for each* subinterval, there are 4 unknowns. So we need $4(N-1)$ equations.\n",
    "\n",
    "Recall when solving ODEs: first we obtain the general solution with **unknown constants**, then we find the particular solution (and the values of the constants) by applying **initial and boundary conditions**. We shall take the same approach here and use conditions for (i) the **value** and (ii) the **slope** of the interpolating function at the subinterval boundaries.\n",
    "\n",
    "**Boundary conditions on function value**\n",
    "\n",
    "We said that the cubic polynomials for each subinterval **must** pass through the bounding data points, $(x_i, y_i)$, exactly. Therefore, $p_i(x_i)=y_i$ and $p_i(x_{i+1})=y_{i+1}$, or\n",
    "\n",
    "$$y_i = a_0^{(i)},\\quad\\quad y_{i+1}=a_0^{(i)} + a_1^{(i)}\\Delta x_i + a_2^{(i)}\\Delta x_i^2 + a_3^{(i)}\\Delta x_i^3,\\quad\\quad \\Delta x_i=x_{i+1}-x_i,\\quad\\quad i=[0,1,\\cdots n-2].$$\n",
    "\n",
    "This gives us $2$ equations for each of the $N-1$ subintervals ($2(N-1)$ equations down, $2(N-1)$ to go...) \n",
    "\n",
    "**Boundary conditions on function derivatives**\n",
    "\n",
    "We also said that the cubic polynomials for each subinterval must have a first and second order derivative that is **continuous** with its neighbours. The first derivative of the polynomial for the $i^{\\text{th}}$ subinterval is:\n",
    "\n",
    "$$ \\frac{dp_i}{dx}=a_1^{(i)}+2a_2^{(i)}(x-x_i)+3a_3^{(i)}(x-x_i)^2 $$\n",
    "\n",
    "and for the $i+1$ subinterval:\n",
    "\n",
    "$$ \\frac{dp_{i+1}}{dx}=a_1^{(i+1)}+2a_2^{(i+1)}(x-x_{i+1})+3a_3^{(i+1)}(x-x_{i+1})^2.$$\n",
    "\n",
    "These two subintervals share a boundary at $x=x_{i+1}$ and here the first derivatives **must** be equal, therefore\n",
    "\n",
    "$$a_1^{(i)}+2a_2^{(i)}\\Delta x_i+3a_3^{(i)}\\Delta x_i^2-a_1^{(i+1)}=0,\\quad\\quad i=[0,1,\\cdots n-2].$$\n",
    "\n",
    "We have one equation of this type for each pair of neighbouring subintervals, i.e., another $N-2$ equations.\n",
    "\n",
    "Applying similar reasoning, but now requiring the **second** derivative to be continuous, leads to another $N-2$ equations of the form\n",
    "\n",
    "$$2a_2^{(i)}+6a_3^{(i)}\\Delta x_i-2a_2^{(i+1)}=0,\\quad\\quad i=[0,1,\\cdots n-2].$$\n",
    "\n",
    "**Finding the last two equations**\n",
    "\n",
    "So far we have used continuity of function value and slope to define $4(N-1)-2$ equations for $4(N-1)$ unknowns. The last two equations, which define a class of cubic splines called **natural** splines, are obtained by requiring the second derivative at the **data extremes** ($x_0$ and $x_{n-1}$) to be zero, i.e.,\n",
    "\n",
    "$$ 2a_2^{(0)}=0, \\quad\\quad 2a_2^{(n-2)}+6a_3^{(n-2)}\\Delta x_{n-2}=0.$$\n",
    "\n",
    "Note, an alternative approach is the \"not-a-knot\" condition, which instead requires the **third** derivative at the first and last subinterval boundaries to be equal.\n",
    "\n",
    "**Putting it all together**\n",
    "The $4(N-1)$ equations in $4(N-1)$ unknowns are found by setting up a matrix equation of the form $A\\mathbf{x}=\\mathbf{b}$, where $\\mathbf{x}$ is the vector of unknown polynomial coefficients, entries in the matrix, $A$, depend only on the terms $c_i$ (the spacing between data points), and the RHS vector, $\\mathbf{b}$, contains only the measurements, $y_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.3.2 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The key algorithm steps for constructing cubic splines are:\n",
    "```\n",
    "1. Set up the matrix, A, populating with appropriate coefficients of the terms in aj^(i) in the equations above.\n",
    "2. Set up the RHS vector, b, populating with the appropriate righthandsides of the equations above.\n",
    "3. Use LU factorization to solve the matrix equation and obtain the spline coefficients.\n",
    "4. Use the spline coefficients to interpolate at the desired points.\n",
    "```\n",
    "Python implementation of this algorithm is the subject of Lab 4. A demonstration is provided in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.3.3 Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run the code in the cell below for a visual illustration of cubic spline interpolation (using Python's built in spline functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sampled_data233 import plot_spline_elements\n",
    "\n",
    "def cubic_spline_figure(interpolate = False, SubIntEqn = 0, xi = None, yi = None, xj = None):\n",
    "    # create figure\n",
    "    f,ax = plt.subplots(1,1)\n",
    "    f.set_size_inches(5,5)\n",
    "    \n",
    "    # see sampling233.py for use of Python built-in spline interpolation\n",
    "    plot_spline_elements(ax, interpolate, SubIntEqn, xi, yi, xj)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(cubic_spline_figure, interpolate = False, SubIntEqn = (0,7,1), xi = fixed(xi), yi = fixed(yi), xj = fixed(xj));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1.3.4 Concept questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***What are the advantages and disadvantages of cubic spline interpolation compared to polynomial fitting and piecewise linear interpolation?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2 Integration](https://en.wikipedia.org/wiki/Numerical_integration)\n",
    "\n",
    "<mark>***Constructing cumulative measures of discrete quantities.***</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need numerical methods for integration? Have a go at solving the integral below. Go ahead, I'll wait.\n",
    "\n",
    "$$ \\int \\frac{\\sin\\left(\\frac{a\\,\\cos(x)+b\\,\\sin(x)}{\\cos(x)}\\right)}{a\\,\\cos(x)+b\\,\\sin(x)}dx$$\n",
    "\n",
    "**Not all integrals can be solved analytically.** Take the general integral $I=\\int\\limits_{x_0}^{x_{n-1}}g(x)\\,dx$, where we know $g(x)$ as the **integrand**.\n",
    "\n",
    "We shall consider a class of methods that approximately evaluate this integral. These methods are based on the idea that the value of an integral, $I$, corresponds to the area under the graph of the integrand. There are two cases:\n",
    "1. We **know** the integrand, $g(x)$, exactly.\n",
    "2. We **don't know** $g(x)$ exactly, but we do have some data, $(x_i, y_i)$. Therefore, we can find an interpolating function, $f(x)\\approx g(x)$.\n",
    "\n",
    "Numerical integration methods break the integration range into manageable sized subintervals (similar to piecewise linear interpolation and cubic splines) and then computes the area of each. If $g(x)$ is known, then the subintervals can be chosen. Otherwise, the subintervals are defined by the data locations $x_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.1 Newton-Cotes methods](https://en.wikipedia.org/wiki/Newton%E2%80%93Cotes_formulas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first consider the **Newton-Cotes methods**, the zeroth, first and second order versions of which you may recognise as the Rectangular, Trapezoidal and Simpson's methods, respectively.\n",
    "\n",
    "These methods approximate the function, $g(x)$, between subinterval boundaries with a polynomial of order, $m$. For example, the Trapezium method fits a straight line between the boundary points $(x_i, g(x_i))$ and $(x_{i+1}, g(x_{i+1}))$.\n",
    "\n",
    "***Run the code in the cell below to see how an integral is approximated by computing the areas of three subintervals. Both cases are considered: a known function, $g(x)$, and collected data, $(x_i, y_i)$.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sampled_data233 import plot_integration_elements\n",
    "\n",
    "def integration_example(gx_known, subintervals, area='None'):\n",
    "    # create figure\n",
    "    f,ax = plt.subplots(1,1)\n",
    "    f.set_size_inches(5,5)\n",
    "    \n",
    "    # show plot\n",
    "    plot_integration_elements(ax, gx_known, subintervals, area)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(integration_example, gx_known=True, subintervals=False, area = ['None', 'A0','A1','A2','Atot']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote bounding values for the $i^{\\text{th}}$ subinterval, $(x_i, y_i)$ and $(x_{i+1}, y_{i+1})$. If the integrand, $g(x)$, is known, then $y_i=g(x_i)$ and $y_{i+1}=g(x_{i+1})$.\n",
    "\n",
    "For the Trapezium rule, the equation of the straight line joining the bounding values is \n",
    "\n",
    "$$ y(x) = \\frac{y_{i+1}-y_i}{x_{i+1}-x_i}(x-x_i)+y_i,$$\n",
    "\n",
    "The area of the subinterval, $A_i$, is then the integral beneath this line, i.e.,\n",
    "\n",
    "$$A_i = \\int\\limits_{x_i}^{x_{i+1}} y(x) dx = \\frac{y_{i+1}+y_i}{2}(x_{i+1}-x_i),$$\n",
    "\n",
    "which might be interpreted literally as \"*the average height of the subinterval, ($\\frac{y_{i+1}+y_i}{2}$), times the width of the subinterval, ($x_{i+1}-x_i$)*\".\n",
    "\n",
    "Imagine that we add together the areas for **two neighbouring subintervals**, $I_i$ and $I_{i+1}$. Suppose further that these subintervals have the same width, i.e., $x_{i+2}-x_{i+1}=x_{i+1}-x_i=\\Delta x$. Recognizing that the expressions for $A_i$ and $A_{i+1}$ share a common boundary point, we can write\n",
    "\n",
    "$$ A_i+A_{i+1} = \\frac{\\Delta x}{2}(y_i+2y_{i+1}+y_{i+2})$$\n",
    "\n",
    "and for $N-1$ subintervals\n",
    "\n",
    "$$I=\\int\\limits_{x_0}^{x_{n-1}} g(x) dx \\,\\,\\approx\\,\\, \\sum\\limits_{i=0}^{n-2}A_i =\\frac{\\Delta x}{2}(y_0+2y_1+\\cdots+2y_{n-2}+y_{n-1}),\\quad\\text{for}\\,\\,y_i=g(x_i)$$.\n",
    "\n",
    "***Exercise 1: Rectangular method***\n",
    "\n",
    "The Rectangular method fits a zeroth order polynomial (a horizontal line) to each subinterval, i.e., $y(x)=y_i$. \n",
    "\n",
    "- ***Show that the area of the $i^{\\text{th}}$ subinterval is given by $A_i=y_i(x_{i+1}-x_i)$.*** \n",
    "- ***Show that the total area of $N-1$ equal-width subintervals is***\n",
    "\n",
    "$$ I \\approx \\Delta x(y_0+y_1+\\cdots+y_{n-2}).$$\n",
    "\n",
    "***Exercise 2: Simpson's method***\n",
    "\n",
    "Simpson's method fits a second order polynomial to each subinterval (with midpoint) $[x_i,\\frac{1}{2}(x_i+x_{i+1}),x_{i+1}]$. \n",
    "\n",
    "- ***For a subinterval of width $\\Delta x$ centered at $x=0$, i.e., $x_i=-\\frac{\\Delta x}{2}$, and $x_{i+1}=\\frac{\\Delta x}{2}$. Show that the quadratic fitting the three function values $[y_i,y_{i+\\frac{1}{2}},y_{i+1}]$, where $y_{i+\\frac{1}{2}}$ is the function evaluated at the subinterval midpoint, is:***\n",
    "\n",
    "$$ y = ax^2+bx+c,\\quad\\text{where}\\,\\,a=\\frac{2}{(\\Delta x)^2}(y_i-2y_{i+\\frac{1}{2}}+y_{i+1}),\\quad b=\\frac{1}{\\Delta x}(y_{i+1}-y_i),\\quad c=y_{i+\\frac{1}{2}}$$\n",
    "\n",
    "- ***Show by integration of $y(x)$ that the area of the subinterval is***\n",
    "\n",
    "$$ I=\\int\\limits_{-\\Delta x/2}^{\\Delta x/2} y(x) dx = \\frac{\\Delta x}{6}(y_i+4y_{i+\\frac{1}{2}}+y_{i+1})$$\n",
    "\n",
    "- ***Thus, show that the total area of $N-1$ equal-width subintervals is***\n",
    "\n",
    "$$ I \\approx \\frac{\\Delta x}{6}(y_0 + 4y_{1/2}+2y_1+4y_{3/2}+\\cdots+2y_{n-2}+4y_{n-3/2}+y_{n-1}). $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sampled_data233 import plot_trapezium_elements\n",
    "\n",
    "def trapezium_figure(N=2):\n",
    "    # create figure\n",
    "    f,ax = plt.subplots(1,1)\n",
    "    f.set_size_inches(5,5)\n",
    "    \n",
    "    # show plot\n",
    "    plot_trapezium_elements(ax, N)\n",
    "\n",
    "# run the interactive figure environment\n",
    "interact(trapezium_figure, N = (2,10,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Concept questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How does increasing the number of subintervals improve the accuracy of the Trapezium method? Describe the tradeoffs.***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Does the trapezium method appear to approximate some subintervals better or worse than others?***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n",
    "\n",
    "***Generalize the Trapezium method to the case of subintervals of uneven width.***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.2 Gaussian quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a numerical scheme for evaluating integrals with polynomial integrands up to a given order, exactly. The polynomial degree that can be approximated depends on the order of the quadrature scheme. For approximating integrals, these methods are preferred over Newton-Cotes methods as they require less function evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start discussing Gaussian quadrature in detail, it is helpful to standardise the integration process. We achieve this by **normalising** the integral.\n",
    "\n",
    "We introduce the variable $\\xi$ such that $\\xi=\\frac{x-x_0}{x_1-x_0}$ and, correspondingly, $x=x_0+(x_1-x_0)\\xi$. From this definition, at $x=x_0$ we have $\\xi=0$ and at $x=x_1$ we have $\\xi=1$. This implies that $dx=(x_1-x_0)d\\xi=Jd\\xi$ (where $J$ is called the **Jacobian** and is the function that transforms $dx\\rightarrow d\\xi$). The integrand, $g(x)$, becomes $g(x_0+(x_1-x_0)\\xi)$. Using these results, we can rewrite the integral, $I$, in normalised form:\n",
    "\n",
    "$$ I=\\int\\limits_{x_0}^{x_1} g(x)\\,dx \\rightarrow \\int\\limits_0^1 \\bar{g}(\\xi)\\,J\\,d\\xi=\\int\\limits_0^1f(\\xi)\\,d\\xi,\\quad\\text{where}\\quad f(\\xi) = g(x_0+(x_1-x_0)\\xi)(x_1-x_0)$$\n",
    "\n",
    "Gaussian quadrature evaluates the normalised integral by approximating it as a **weighted sum of function evaluations at particular locations**. *Both* the weights *and* the particular locations are the key to this method. Expressing the previous statement:\n",
    "\n",
    "$$ \\int\\limits_0^1 f(\\xi)\\,d\\xi = \\sum\\limits_{g=0}^{G-1} w_g\\,f\\left(\\xi^{(g)}\\right) +E_G= w_0\\,f\\left(\\xi^{(0)}\\right)+ w_1\\,f\\left(\\xi^{(1)}\\right)+\\cdots+ w_{G-1}\\,f\\left(\\xi^{(G-1)}\\right)+E_G,$$\n",
    "\n",
    "where the sample locations $f\\left(\\xi^{(g)}\\right)$ are called **Gauss points**, $w_g$ are called the **weights** and $E_G$ is the error associated with the approximation. To implement the method, we need to: (1) find the Gauss point locations, $\\xi^{(g)}$, (2) evaluate the integrand at these locations, $f\\left(\\xi^{(g)}\\right)$, and (3) find the weights, $w_g$, to sum up the evaluations.\n",
    "\n",
    "It can be shown that choosing $G$ Gauss points allows a polynomial of order $2G-1$ to be integrated exactly, i.e., $E_G=0$ ($G$ Gauss points have $2G$ unknowns - two locations and two weights - and a $2G-1$ polynomial has $2G$ unknown coefficients). \n",
    "\n",
    "Imagine that we will use a two-point scheme ($G=2$) to integrate exactly the cubic polynomial, $f(\\xi) = a\\xi^3+b\\xi^2+c\\xi+d$. Then we can write\n",
    "\n",
    "$$\\int\\limits_0^1 f(\\xi)\\,d\\xi = w_0\\,f\\left(\\xi^{(0)}\\right)+w_1\\,f\\left(\\xi^{(1)}\\right).$$\n",
    "\n",
    "Substituting the cubic expression of $f$ and simplifying the integrals gives\n",
    "\n",
    "$$ a\\int\\limits_0^1 \\xi^3\\,d\\xi + b\\int\\limits_0^1 \\xi^2\\,d\\xi + c\\int\\limits_0^1 \\xi\\,d\\xi+ d\\int\\limits_0^1\\,d\\xi = w_0\\left(a\\left(\\xi^{(0)}\\right)^3+b\\left(\\xi^{(0)}\\right)^2+c\\left(\\xi^{(0)}\\right)+d\\right)+w_1\\left(a\\left(\\xi^{(1)}\\right)^3+b\\left(\\xi^{(1)}\\right)^2+c\\left(\\xi^{(1)}\\right)+d\\right),$$\n",
    "\n",
    "which **must be true** for any combination of the coefficients $[a,b,c,d]$. Let's pick some values then that make our lives easier:\n",
    "\n",
    "$$ \\text{for}\\quad a=b=c=0,\\quad d=1 \\quad \\rightarrow \\quad \\int\\limits_0^1\\,d\\xi= w_0+w_1,$$\n",
    "\n",
    "which, when the integral is evaluated, yields $w_0+w_1=1$.\n",
    "\n",
    "***Exercises***\n",
    "\n",
    "- ***Show that, by setting each of the coefficients $a$, $b$, and $c$ in turn to $1$, and holding the others as $0$, we obtain the three additional equations below:***\n",
    "\n",
    "$$ \\frac{1}{2}=w_0\\,\\xi^{(0)}+w_1\\,\\xi^{(1)}$$\n",
    "\n",
    "$$ \\frac{1}{3}=w_0\\,\\left(\\xi^{(0)}\\right)^2+w_1\\,\\left(\\xi^{(1)}\\right)^2$$\n",
    "\n",
    "$$ \\frac{1}{4}=w_0\\,\\left(\\xi^{(0)}\\right)^3+w_1\\,\\left(\\xi^{(1)}\\right)^3$$\n",
    "\n",
    "- ***Show by solving these equations for the four unknowns, $w_0$, $w_1$, $\\xi^{(0)}$ and $\\xi^{(1)}$, that***\n",
    "\n",
    "$$ w_0=w_1=\\frac{1}{2},\\quad\\text{and}\\quad\\xi^{(0)},\\xi^{(1)}=\\frac{1}{2}\\mp\\frac{1}{2\\sqrt{3}}$$.\n",
    "\n",
    "- ***Use Gaussian quadrature to evaluate the two integrals below and compare your result to the traditional method (integrate, apply limits). When do the results agree and when do they disagree?***\n",
    "\n",
    "$$ I_0 = \\int\\limits_{0}^{1} 3x^2\\, dx,\\quad\\quad I_1 = \\int\\limits_{0}^{1} 5x^4\\, dx, $$\n",
    "\n",
    "***Higher order schemes***\n",
    "\n",
    "A fifth degree polynomial may be integrated exactly using 3 Gauss points with:\n",
    "\n",
    "$$ w_0=w_2=\\frac{5}{18},\\quad w_1 = \\frac{4}{9},\\quad\\text{and}\\quad \\xi^{(0)},\\xi^{(2)}=\\frac{1}{2}\\mp\\frac{1}{2}\\sqrt{\\frac{3}{5}}, \\quad \\xi^{(1)}=\\frac{1}{2}.$$\n",
    "\n",
    "Indeed, it is a **general feature** of these sorts of Gauss quadrature schemes that (i) the positions of the Gauss points are **symmetric** about $\\frac{1}{2}$, and that (ii) the weights reflect this symmetry, and further (iii) the weights **sum to 1**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we demonstrate Gaussian quadrature for some already normalized integrals.\n",
    "w0 = 0.5\n",
    "w1 = 0.5\n",
    "xi0 = 0.5-1./(2.*np.sqrt(3))\n",
    "xi1 = 0.5+1./(2.*np.sqrt(3))\n",
    "\n",
    "# functions for x**2\n",
    "def f(x): return x**2        # a function\n",
    "def I(x): return x**3/3.     # its exact integral\n",
    "\n",
    "#def f(x): return x**3\n",
    "#def I(x): return x**4/4.\n",
    "\n",
    "#def f(x): return x**4\n",
    "#def I(x): return x**5/5.\n",
    "\n",
    "#def f(x): return np.sin(x)\n",
    "#def I(x): return -np.cos(x)\n",
    "\n",
    "#n = 2\n",
    "#def f(x): return np.sin(n*x)\n",
    "#def I(x): return -np.cos(n*x)/n\n",
    "\n",
    "# integrate f(x) between 0 and 1 exactly\n",
    "print('exact =',I(1) - I(0))\n",
    "\n",
    "# Gaussian quadrature\n",
    "print('approx=',w0*f(xi0)+w1*f(xi1))\n",
    "\n",
    "# a plot of f(x)\n",
    "fig,ax = plt.subplots(1,1)\n",
    "x = np.linspace(0,1,1001)\n",
    "ax.plot(x, f(x), 'b-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***For the case of the 4th order polynomial, modify the quadrature above to implement a 3-point rule, and show that the approximation is exact.***\n",
    "\n",
    "***Does a 3-point rule generate an exact approximation for `sin(nx)`? Comment.***\n",
    "\n",
    "> <mark>*~ your answer here ~*</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***- - - - CLASS CODING EXERCISE - - - -***\n",
    "\n",
    "Imagine we wish to evaluate the integral\n",
    "\n",
    "\\begin{equation}\n",
    "I = \\int\\limits_0^\\pi sin(x)\\,dx\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART ONE\n",
    "# --------\n",
    "# WRITE down the normalised integral using the first equation in 2.2.1\n",
    "\n",
    "# f(\\xi) = \n",
    "\n",
    "# What is the ANALYTICAL solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART TWO\n",
    "# --------\n",
    "# IMPLEMENT the normalized function below\n",
    "# **hint** use np.pi and np.sin()\n",
    "def sin_norm(xi):\n",
    "    return ????\n",
    "\n",
    "# Use GAUSS quadrature to compute the integral.\n",
    "Iapprox = ????\n",
    "Itrue = ????\n",
    "print(Iapprox)\n",
    "print('error=',Iapprox-Itrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL CHALLENGE\n",
    "# ------------------\n",
    "# Improve your estimate I_approx by\n",
    "# 1. dividing the integral into two parts \n",
    "#    I = I1 + I2 = int_0^pi/2 ... dx + int_pi/2^pi ... dx\n",
    "# 2. do Gaussian quadrature on each \n",
    "\n",
    "def sin_norm2(xi):\n",
    "    return ????\n",
    "\n",
    "I_approx = ????\n",
    "I_true = ????\n",
    "print('error=', I_approx-I_true)\n",
    "\n",
    "# this is called a COMPOSITE quadrature scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key algorithm steps to implement Gaussian quadrature are:\n",
    "```\n",
    "1. Normalise the integrand.\n",
    "2. Determine Gauss points and weights for appropriate scheme.\n",
    "3. Evaluate normalised integrand at Gauss points.\n",
    "4. Compute the weighted sum.\n",
    "```\n",
    "\n",
    "You will implement these steps in the lab."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
